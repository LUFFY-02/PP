KNN

K-Nearest Neighbors (KNN) is a simple yet powerful supervised learning algorithm used for classification and regression tasks. It's considered a non-parametric and lazy learning algorithm because it doesn't make any assumptions about the underlying data distribution, and it doesn't explicitly learn a model during training. Instead, it memorizes the training dataset and makes predictions based on the similarity of new data points to existing data points in the training set.

Here are the main steps of the K-Nearest Neighbors algorithm:

1. **Choose the Value of K**: Determine the number of nearest neighbors (K) to consider when making predictions. This value is typically chosen empirically or through cross-validation.

2. **Calculate Distance**: Compute the distance between the new data point (query point) and all data points in the training set. The most commonly used distance metrics are Euclidean distance, Manhattan distance, or Minkowski distance.

3. **Find K Nearest Neighbors**: Identify the K nearest neighbors of the new data point based on the computed distances. These are the data points with the smallest distances to the query point.

4. **Majority Vote (Classification) or Weighted Average (Regression)**:
    - For classification: Assign the class label that occurs most frequently among the K nearest neighbors. This is often referred to as majority voting.
    - For regression: Calculate the weighted average of the target values of the K nearest neighbors. The weights can be inversely proportional to the distance from the query point.

5. **Make Prediction**: Assign the predicted class label (for classification) or predicted value (for regression) to the new data point based on the outcome of the majority vote or weighted average.

6. **Evaluate Performance**: Assess the performance of the KNN algorithm using appropriate evaluation metrics such as accuracy, precision, recall, F1-score (for classification), or mean squared error (for regression). This step is typically done using a separate validation set or through cross-validation.

7. **Optimize Hyperparameters (Optional)**: Fine-tune the hyperparameters of the KNN algorithm, such as the value of K or the choice of distance metric, to improve performance on unseen data. This can be done through techniques like grid search or randomized search.

8. **Predict on Test Data**: Once the model is trained and optimized, use it to make predictions on new, unseen data points. This step evaluates the generalization ability of the model.

By following these steps, the K-Nearest Neighbors algorithm can be effectively applied to classification and regression tasks, making it a versatile and widely used machine learning technique.

__________________________________________________

Sure, here's a basic implementation of K-Nearest Neighbors (KNN) in C++ with OpenMP for parallelization:

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <algorithm>
#include <omp.h>

using namespace std;

// Structure to represent a data point
struct DataPoint {
    vector<double> features;
    int label;
};

// Function to calculate Euclidean distance between two data points
double calculateDistance(const vector<double>& p1, const vector<double>& p2) {
    double sum = 0.0;
    for (size_t i = 0; i < p1.size(); ++i) {
        double diff = p1[i] - p2[i];
        sum += diff * diff;
    }
    return sqrt(sum);
}

// Function to perform KNN classification
int knn(const vector<DataPoint>& trainData, const vector<double>& testData, int k) {
    vector<pair<double, int>> distances; // Pair of (distance, label)
    distances.reserve(trainData.size());

    // Calculate distances between test point and all training points
    #pragma omp parallel for
    for (size_t i = 0; i < trainData.size(); ++i) {
        double dist = calculateDistance(trainData[i].features, testData);
        distances.push_back({dist, trainData[i].label});
    }

    // Sort distances in ascending order
    sort(distances.begin(), distances.end());

    // Count the occurrences of each label among the k nearest neighbors
    vector<int> labelCount(10, 0); // Assuming labels are from 0 to 9
    for (int i = 0; i < k; ++i) {
        int label = distances[i].second;
        labelCount[label]++;
    }

    // Find the majority class
    int maxCount = 0;
    int predictedLabel = -1;
    for (int label = 0; label < 10; ++label) {
        if (labelCount[label] > maxCount) {
            maxCount = labelCount[label];
            predictedLabel = label;
        }
    }

    return predictedLabel;
}

int main() {
    // Example dataset
    vector<DataPoint> trainData = {
        {{1.2, 2.3, 3.4}, 0},
        {{2.1, 3.2, 4.3}, 1},
        {{3.4, 4.5, 5.6}, 0},
        // Add more training data here
    };

    vector<double> testData = {1.5, 2.5, 3.5}; // Example test data

    int k = 3; // Number of neighbors to consider

    // Perform KNN classification
    int predictedLabel = knn(trainData, testData, k);

    // Output the predicted label
    cout << "Predicted label: " << predictedLabel << endl;

    return 0;
}
```

This code includes the following steps:

1. Define a `DataPoint` structure to represent each data point, consisting of features and a label.
2. Implement a function `calculateDistance` to compute the Euclidean distance between two data points.
3. Implement the `knn` function to perform KNN classification. This function calculates distances between the test point and all training points, sorts them, counts the occurrences of each label among the k nearest neighbors, and predicts the label based on the majority class.
4. In the `main` function, define the training and test datasets, specify the value of k, and call the `knn` function to perform classification. Finally, output the predicted label.

OpenMP is used to parallelize the distance calculations loop, enabling efficient computation on multiple threads.

______________________________________________________________________________

Sure, here's a simple example of applying K-Nearest Neighbors (KNN) algorithm in C++ with OpenMP. We'll create a struct `Car` to represent each car, generate random data for age and distance traveled, and then classify a new input as "good" or "bad" based on the nearest neighbors:

```cpp
#include <iostream>
#include <vector>
#include <random>
#include <cmath>
#include <omp.h>

using namespace std;

// Structure to represent a car
struct Car {
    double age;
    double distanceTravelled;
    string quality;
};

// Function to generate random double between min and max
double randomDouble(double min, double max) {
    static thread_local mt19937 generator(omp_get_thread_num());
    uniform_real_distribution<double> distribution(min, max);
    return distribution(generator);
}

// Function to generate random dataset of cars
vector<Car> generateRandomData(int size) {
    vector<Car> data;
    for (int i = 0; i < size; ++i) {
        Car car;
        car.age = randomDouble(0.0, 20.0);  // Age between 0 and 20 years
        car.distanceTravelled = randomDouble(0.0, 300000.0);  // Distance between 0 and 300000 km
        car.quality = (randomDouble(0.0, 1.0) < 0.5) ? "good" : "bad";  // Randomly assign quality
        data.push_back(car);
    }
    return data;
}

// Function to classify a new input using KNN
string classify(const vector<Car>& data, double newAge, double newDistance, int k) {
    vector<pair<double, string>> distances;
    // Calculate distances to all data points
    for (const auto& car : data) {
        double distance = sqrt(pow(newAge - car.age, 2) + pow(newDistance - car.distanceTravelled, 2));
        distances.push_back({distance, car.quality});
    }
    // Sort distances
    sort(distances.begin(), distances.end());
    // Count "good" and "bad" labels among the nearest neighbors
    int goodCount = 0, badCount = 0;
    for (int i = 0; i < k; ++i) {
        if (distances[i].second == "good") {
            goodCount++;
        } else {
            badCount++;
        }
    }
    // Return the majority label
    return (goodCount > badCount) ? "good" : "bad";
}

int main() {
    int dataSize = 100000;
    vector<Car> data = generateRandomData(dataSize);

    double newAge = randomDouble(0.0, 20.0);
    double newDistance = randomDouble(0.0, 300000.0);
    int k = 5;  // Number of nearest neighbors to consider

    string result = classify(data, newAge, newDistance, k);
    cout << "For a car with age " << newAge << " years and distance travelled " << newDistance << " km, quality is predicted as " << result << endl;

    return 0;
}
```

In this code:

- We define the `Car` struct to represent each car, with age, distance travelled, and quality (good or bad).
- We generate random data for age and distance travelled using the `generateRandomData` function.
- The `classify` function calculates the Euclidean distance between the new input and all data points, sorts the distances, and finds the majority label among the k nearest neighbors.
- In the `main` function, we generate random new input (age and distance travelled) and classify it using KNN with k=5. Finally, we output the predicted quality.